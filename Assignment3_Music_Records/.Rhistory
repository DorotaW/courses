plot(calDate, cal, type = "h", lwd = 5, col = rgb(0, 0.5, 1),
xlim = dlim, ylim = c(0, max(1.1*cal,1.1*eTDEE) ),
xlab = "", ylab = "Net Calories")
axis.Date(side = 4, dlim, lab = FALSE)
# average calories per day
CPD <- mean(cal)
lines(calDate,rollCDP, lty = 2)
abline(h = eTDEE, lty = 2,col='red')
abline(h = goal , lty = 2,col='green')
usr <- par("usr"); cxy <- par("cxy")
text(usr[1], eTDEE - 0.5*cxy[2],
paste("Average calores per day:", round(CPD)), pos = 4)
text(usr[1], eTDEE + 0.5*cxy[2],
paste("TDEE:", round(eTDEE)), pos = 4, col='red')
text(usr[1], goal - 0.5*cxy[2],
paste("Goal:", goal), pos = 4, col='green')
par(mar = c(4, 4, 1, 1) )
plot(wtDate, wt, type = "n",
xlim = dlim, ylim = c(75, max(wt)+1 ),
xlab = "Date", ylab = "Weight (kg)")
axis.Date(side = 4, dlim, lab = FALSE)
m <- lm(wt~wtDate)
abline(m, lty = 2, col = "gray" )
points(wtDate, wt, pch = 21, bg='black', xpd = NA)
usr <- par("usr"); cxy <- par("cxy")
# weight loss per week:
WLPW <- -(m$coefficients[2] * 7)
text(usr[2], usr[4] - cxy[2]*0.5,
paste("Weight loss per week:", round(WLPW, 2),"kg"), pos = 2, col='blue')
# estimate TDEE
TDEE <- CPD + WLPW*1100
text(usr[1], usr[3] + cxy[2]*0.5,
paste("TDEE =", round(TDEE)),col='red', pos = 4)
plot(rollCDP,rollWTPD,bg='black', pch = 21,ylim=c(-2,2), xlim=range(rollCDP)+c(-500,500),
xlab = "Net Calories", ylab = "Weight Loss (kg)")
md <- lm(rollWTPD~rollCDP)
abline(md, lty = 2, col = "gray" )
xcals <- xmlParse("cals30.xml")
library(XML)
xcals <- xmlParse("cals30.xml")
xwt <- xmlParse("weight30.xml")
xcals <- xmlParse("cals30.xml")
library(XML)
xcals <- xmlParse("cals30.xml")
fileURL <- "http://www.myfitnesspal.com/reports/results/nutrition/Net%20Calories/30"
xData <- getURL(fileURL)
xcals <- xmlParse(xData)
xData <- getURL(fileURL)
library(XML)
library(RCurl)
install.packages("RCurl")
library(XML)
library(RCurl)
fileURL <- "http://www.myfitnesspal.com/reports/results/nutrition/Net%20Calories/30"
xData <- getURL(fileURL)
xcals <- xmlParse(xData)
lcals <- xmlToList(xcals)
xcals <- xmlParse("cals30.xml")
xwt <- xmlParse("weight30.xml")
xcals <- xmlParse("cals30.xml")
xwt <- xmlParse("weight30.xml")
library('knitr')
library("knitr", lib.loc="~/R/win-library/3.1")
job: Johns Hopkins Bloomberg School of Public Health
---
title: "Introduction to regression"
author: "Brian Caffo, Jeff Leek and Roger Peng"
highlighter: highlight.js
output: html_document
job: Johns Hopkins Bloomberg School of Public Health
logo: bloomberg_shield.png
mode: selfcontained
hitheme: tomorrow
subtitle: Regression
framework: io2012
url:
assets: ../../assets
lib: ../../librariesNew
widgets: mathjax
---
library('knitr')
```{r setup, cache = F, echo = F, message = F, warning = F, tidy = F, results='hide'}
# make this an external chunk that can be included in any file
options(width = 100)
opts_chunk$set(message = F, error = F, warning = F, comment = NA, fig.align = 'center', dpi = 100, tidy = F, cache.path = '.cache/', fig.path = 'fig/')
options(xtable.type = 'html')
knit_hooks$set(inline = function(x) {
if(is.numeric(x)) {
round(x, getOption('digits'))
} else {
paste(as.character(x), collapse = ', ')
}
})
knit_hooks$set(plot = knitr:::hook_plot_html)
runif(1)
```
## A famous motivating example
<img class=center src=fig/galton.jpg height=150>
### (Perhaps surprisingly, this example is still relevant)
<img class=center src=fig/height.png height=150>
[http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html](http://www.nature.com/ejhg/journal/v17/n8/full/ejhg20095a.html)
[Predicting height: the Victorian approach beats modern genomics](http://www.wired.com/wiredscience/2009/03/predicting-height-the-victorian-approach-beats-modern-genomics/)
---
## Recent simply statistics post
(Simply Statistics is a blog by Jeff Leek, Roger Peng and
Rafael Irizarry, who wrote this post, link on the image)
<a href="http://simplystatistics.org/2013/01/28/data-supports-claim-that-if-kobe-stops-ball-hogging-the-lakers-will-win-more/">
<img class=center src=http://simplystatistics.org/wp-content/uploads/2013/01/kobelakers1-1024x1024.png height=250></img>
</a>
- "Data supports claim that if Kobe stops ball hogging the Lakers will win more"
- "Linear regression suggests that an increase of 1% in % of shots taken by Kobe results in a drop of 1.16 points (+/- 0.22)  in score differential."
- How was it done? Do you agree with the analysis?
---
## Questions for this class
* Consider trying to answer the following kinds of questions:
* To use the parents' heights to predict childrens' heights.
* To try to find a parsimonious, easily described mean
relationship between parent and children's heights.
* To investigate the variation in childrens' heights that appears
unrelated to parents' heights (residual variation).
* To quantify what impact genotype information has beyond parental height in explaining child height.
* To figure out how/whether and what assumptions are needed to
generalize findings beyond the data in question.
* Why do children of very tall parents tend to be
tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little taller than their parents? (This is a famous question called 'Regression to the mean'.)
---
## Galton's Data
* Let's look at the data first, used by Francis Galton in 1885.
* Galton was a statistician who invented the term and concepts
of regression and correlation, founded the journal Biometrika,
and was the cousin of Charles Darwin.
* You may need to run `install.packages("UsingR")` if the `UsingR` library is not installed.
* Let's look at the marginal (parents disregarding children and children disregarding parents) distributions first.
* Parent distribution is all heterosexual couples.
* Correction for gender via multiplying female heights by 1.08.
* Overplotting is an issue from discretization.
---
```{r galton,fig.height=3.5,fig.width=8}
library(UsingR); data(galton); library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth=1)
g <- g + facet_grid(. ~ variable)
g
```
---
## Finding the middle via least squares
* Consider only the children's heights.
* How could one describe the "middle"?
* One definition, let $Y_i$ be the height of child $i$ for $i = 1, \ldots, n = 928$, then define the middle as the value of $\mu$
that minimizes $$\sum_{i=1}^n (Y_i - \mu)^2$$
* This is physical center of mass of the histrogram.
* You might have guessed that the answer $\mu = \bar Y$.
---
## Experiment
### Use R studio's manipulate to see what value of $\mu$ minimizes the sum of the squared deviations.
```
library(manipulate)
myHist <- function(mu){
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mu, size = 3)
g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```
---
## The least squares est. is the empirical mean
```{r , fig.height=4, fig.width=4, fig.align='center'}
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mean(galton$child), size = 3)
g
```
---
### The math (not required for the class) follows as:
$$
\begin{align}
\sum_{i=1}^n \left(Y_i - \mu\right)^2 & = \
\sum_{i=1}^n \left(Y_i - \bar Y + \bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \sum_{i=1}^n \left(Y_i - \bar Y\right)  \left(\bar Y - \mu\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \left(\bar Y - \mu\right) \sum_{i=1}^n \left(Y_i - \bar Y\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \left(\bar Y - \mu\right)  \left(\left(\sum_{i=1}^n Y_i\right) -\
n \bar Y\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
\sum_{i=1}^n \left(\bar Y - \mu\right)^2\\
& \geq \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 \
\end{align}
$$
---
## Comparing childrens' heights and their parents' heights
```{r, dependson="galton",fig.height=4,fig.width=4, fig.align='center'}
ggplot(galton, aes(x = parent, y = child)) + geom_point()
```
---
Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).
```{r freqGalton, dependson="galton",fig.height=6,fig.width=7,echo=FALSE}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
```
---
## Regression through the origin
* Suppose that $X_i$ are the parents' heights.
* Consider picking the slope $\beta$ that minimizes $$\sum_{i=1}^n (Y_i - X_i \beta)^2$$
* This is exactly using the origin as a pivot point picking the
line that minimizes the sum of the squared vertical distances
of the points to the line
* Use R studio's  manipulate function to experiment
* Subtract the means so that the origin is the mean of the parent
and children's heights
---
```{r, echo = TRUE, eval = FALSE}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
mse <- mean( (y - beta * x) ^2 )
g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```
---
## The solution
### In the next few lectures we'll talk about why this is the solution
```{r}
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```
---
```{r, fig.height=6,fig.width=7,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
```
install.packages("UsingR")
library("UsingR", lib.loc="~/R/win-library/3.1")
library(UsingR); data(galton); library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth=1)
g <- g + facet_grid(. ~ variable)
g
g <- ggplot(long, aes(x = value, fill = variable))
long <- melt(galton)
library(UsingR)
data(galton)
install.packages("reshape")
* Consider only the children's heights.
install.packages("dplyr")
library('knitr')
library('knitr')
install.packages("ggplot2")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
x<- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
lsfit(x,w)
lsfit(x,wt=w)
x*w
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(I(x-mean(x))~I(y-mean(y)))
lm(y~x)
data(mtcarts)
data(mtcars)
view(mtcars)
d<-data(mtcars)
library("datasets", lib.loc="C:/Program Files/R/R-3.1.3/library")
d<-data(mtcars)
data(mtcars)
mtcars
lm(x~y)
lm(mpg~wt)
lm(mpg~wt,data=mtcars)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(stdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(std(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sd(a))
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
mean(z)
mean(x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<lm(y~x)
f<-lm(y~x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<-lm(y~x)
summary(f)
mtcars
m<-lm(mpg~wt,data=mtcars)
summary(m)
-5.3445-0.55591*1.645
-5.3445-0.55591*1.645+37.2
(-5.3445-0.55591*1.645)*mean(mtcars.wt)+37.2
(-5.3445-0.55591*1.645)*mean(mtcars$wt)+37.2
(-5.3445+0.55591*1.645)*3000+37.2
(-5.3445+0.55591*1.645)*3+37.2
(-5.3445+0.55591*1.96)*3+37.2
(-5.3445)*3+37.2+1.645*3.046
(-5.3445)*mean(mtcars$wt)+37.2-1.645*3.046
predict(m, data.frame(mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=3), interval="confidence")
summary(m)
(-5.3445)*2-2*1.645*0.5591
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training)
hist(training$SuperPlasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
setwd("C:/Users/Dorota/Github/The_Analytics_Edge/Assignment3_Music_Records")
data <- read.csv("songs.csv", header = TRUE)
data[which(artistname='Michael Jackson')]
data[which(data$artistname='Michael Jackson')]
data[which(data$artistname=='Michael Jackson')]
data[which(data$artistname=='Michael Jackson'),]
data[which(data$artistname=='Michael Jackson' & data$Top10==1),]
data <- read.csv("songs.csv", header = TRUE)
data[which(data$artistname=='Michael Jackson' & data$Top10==1),2]
table(data$timesignature)
data[order(-tempo),2]
data[order(-data$tempo),c(2,9)]
data[order(-data$tempo),c(2,9)][1]
dataord<-data[order(-data$tempo),c(2,9)]
dataord[1,]
training<-subset(data,Year<=2009)
testing<-subset(data,Year>2009)
training<-subset(data,year<=2009)
testing<-subset(data,year>2009)
SongsTrain<-subset(data,year<=2009)
SongsTest<-subset(data,year>2009)
nonvars = c("year", "songtitle", "artistname", "songID", "artistID")
SongsTrain = SongsTrain[ , !(names(SongsTrain) %in% nonvars) ]
SongsTest = SongsTest[ , !(names(SongsTest) %in% nonvars) ]
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
summary(SongsLog1)
cor(data$loudness,data$energy)
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
summary(SongsLog2)
SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)
summary(SongsLog3)
confusionMatrix(predict(SongsLog3,SongsTest),SongsTest$Top10)
library(caret)
confusionMatrix(predict(SongsLog3,SongsTest),SongsTest$Top10)
if (predict(SongsLog3,SongsTest)>0.45) 1 else 0
predict(SongsLog3,SongsTest)
(predict(SongsLog3,SongsTest)>0.45)
cases(
1=(predict(SongsLog3,SongsTest)>=0.45),
0=TRUE)
cases(
"1"=(predict(SongsLog3,SongsTest)>=0.45),
"0"=TRUE)
install.packages("memisc")
library(memisc)
predTop10<-cases(
"1"=(predict(SongsLog3,SongsTest)>=0.45),
"0"=TRUE)
predTop10<-factor(predTop10)
predTop10<-factor(predTop10)
predTop10<-int(predTop10)
predTop10<-as.numeric(predTop10)
predTop10<-as.numeric(as.character(predTop10))
confusionMatrix(predTop10,SongsTest$Top10)
predTop10<-cases(
"1"=(predict(SongsLog3,SongsTest)>=0.45),
"0"=TRUE)
confusionMatrix(predTop10,SongsTest$Top10)
predTop10<-cases(
"1"=(predict(SongsLog3,SongsTest)>0.45),
"0"=TRUE)
predTop10<-as.numeric(as.character(predTop10))
confusionMatrix(predTop10,SongsTest$Top10)
(314+7)/(314+52+7)
314/(314+52+7)
predict(SongsLog3,SongsTest)
predTop10<-cases(
"1"=(exp(predict(SongsLog3,SongsTest))>0.45),
"0"=TRUE)
predTop10<-as.numeric(as.character(predTop10))
confusionMatrix(predTop10,SongsTest$Top10)
(290+28)/(314+52+7)
predTop10<-cases(
"1"=(exp(predict(SongsLog3,SongsTest))>=0.45),
"0"=TRUE)
predTop10<-as.numeric(as.character(predTop10))
confusionMatrix(predTop10,SongsTest$Top10)
predTop10<-cases(
"1"=(predict(SongsLog3,SongsTest,type="response")>=0.45),
"0"=TRUE)
predTop10<-as.numeric(as.character(predTop10))
confusionMatrix(predTop10,SongsTest$Top10)

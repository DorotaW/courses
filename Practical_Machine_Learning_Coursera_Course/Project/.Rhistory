Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).
```{r freqGalton, dependson="galton",fig.height=6,fig.width=7,echo=FALSE}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
```
---
## Regression through the origin
* Suppose that $X_i$ are the parents' heights.
* Consider picking the slope $\beta$ that minimizes $$\sum_{i=1}^n (Y_i - X_i \beta)^2$$
* This is exactly using the origin as a pivot point picking the
line that minimizes the sum of the squared vertical distances
of the points to the line
* Use R studio's  manipulate function to experiment
* Subtract the means so that the origin is the mean of the parent
and children's heights
---
```{r, echo = TRUE, eval = FALSE}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
mse <- mean( (y - beta * x) ^2 )
g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```
---
## The solution
### In the next few lectures we'll talk about why this is the solution
```{r}
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```
---
```{r, fig.height=6,fig.width=7,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
```
install.packages("UsingR")
library("UsingR", lib.loc="~/R/win-library/3.1")
library(UsingR); data(galton); library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth=1)
g <- g + facet_grid(. ~ variable)
g
g <- ggplot(long, aes(x = value, fill = variable))
long <- melt(galton)
library(UsingR)
data(galton)
install.packages("reshape")
* Consider only the children's heights.
install.packages("dplyr")
library('knitr')
library('knitr')
install.packages("ggplot2")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
x<- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
lsfit(x,w)
lsfit(x,wt=w)
x*w
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(I(x-mean(x))~I(y-mean(y)))
lm(y~x)
data(mtcarts)
data(mtcars)
view(mtcars)
d<-data(mtcars)
library("datasets", lib.loc="C:/Program Files/R/R-3.1.3/library")
d<-data(mtcars)
data(mtcars)
mtcars
lm(x~y)
lm(mpg~wt)
lm(mpg~wt,data=mtcars)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(stdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(std(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sd(a))
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
mean(z)
mean(x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<lm(y~x)
f<-lm(y~x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<-lm(y~x)
summary(f)
mtcars
m<-lm(mpg~wt,data=mtcars)
summary(m)
-5.3445-0.55591*1.645
-5.3445-0.55591*1.645+37.2
(-5.3445-0.55591*1.645)*mean(mtcars.wt)+37.2
(-5.3445-0.55591*1.645)*mean(mtcars$wt)+37.2
(-5.3445+0.55591*1.645)*3000+37.2
(-5.3445+0.55591*1.645)*3+37.2
(-5.3445+0.55591*1.96)*3+37.2
(-5.3445)*3+37.2+1.645*3.046
(-5.3445)*mean(mtcars$wt)+37.2-1.645*3.046
predict(m, data.frame(mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=3), interval="confidence")
summary(m)
(-5.3445)*2-2*1.645*0.5591
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training)
hist(training$SuperPlasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(parallel)
library(doParallel)
#load raw data
data <- read.csv("pml-training.csv", header = TRUE)
assignment  <- read.csv('pml-testing.csv')
#store classe as factor
data$classe <- factor(data$classe)
data<-data[,-c(1,3,4,5,6,7)]
table(data$user_name)
table(assignment$user_name)
#split data in 60% training and 40% test
set.seed(9876)
trainIndex <- createDataPartition(y = data$classe, p=0.6,list=FALSE)
training <- data[trainIndex,]
testing<- data[-trainIndex,]
#Pre processing
NAColumn<- apply(training,2,function(x) {sum(is.na(x))})
preProcValues<-preProcess(training[,NAColumn>0], method="medianImpute")
trainProc <- predict(preProcValues, training)
testProc <- predict(preProcValues, testing)
nzv <- nearZeroVar(trainProc)
trainProc<- trainProc[, -nzv]
testProc<- testProc[, -nzv]
dummy<-dummyVars(~user_name, data=trainProc)
trainProc<-cbind(predict(dummy,trainProc),trainProc[-1]))
testProc<-cbind(predict(dummy,testProc),testProc[-1]))
library(caret)
library(parallel)
library(doParallel)
#load raw data
data <- read.csv("pml-training.csv", header = TRUE)
assignment  <- read.csv('pml-testing.csv')
#store classe as factor
data$classe <- factor(data$classe)
data<-data[,-c(1,3,4,5,6,7)]
table(data$user_name)
table(assignment$user_name)
#split data in 60% training and 40% test
set.seed(9876)
trainIndex <- createDataPartition(y = data$classe, p=0.6,list=FALSE)
training <- data[trainIndex,]
testing<- data[-trainIndex,]
#Pre processing
NAColumn<- apply(training,2,function(x) {sum(is.na(x))})
preProcValues<-preProcess(training[,NAColumn>0], method="medianImpute")
trainProc <- predict(preProcValues, training)
testProc <- predict(preProcValues, testing)
nzv <- nearZeroVar(trainProc)
trainProc<- trainProc[, -nzv]
testProc<- testProc[, -nzv]
dummy<-dummyVars(~user_name, data=trainProc)
trainProc<-cbind(predict(dummy,trainProc),trainProc[-1]))
testProc<-cbind(predict(dummy,testProc),testProc[-1]))
library(caret)
library(parallel)
library(doParallel)
#load raw data
data <- read.csv("pml-training.csv", header = TRUE)
assignment  <- read.csv('pml-testing.csv')
#store classe as factor
data$classe <- factor(data$classe)
data<-data[,-c(1,3,4,5,6,7)]
table(data$user_name)
table(assignment$user_name)
#split data in 60% training and 40% test
set.seed(9876)
trainIndex <- createDataPartition(y = data$classe, p=0.6,list=FALSE)
training <- data[trainIndex,]
testing<- data[-trainIndex,]
#Pre processing
NAColumn<- apply(training,2,function(x) {sum(is.na(x))})
preProcValues<-preProcess(training[,NAColumn>0], method="medianImpute")
trainProc <- predict(preProcValues, training)
testProc <- predict(preProcValues, testing)
nzv <- nearZeroVar(trainProc)
trainProc<- trainProc[, -nzv]
testProc<- testProc[, -nzv]
dummy<-dummyVars(~user_name, data=trainProc)
setwd("C:/Users/Dorota/Github/Data_Science_Specialization/Practical_Machine_Learning_Coursera_Course/Project")
library(caret)
library(parallel)
library(doParallel)
#load raw data
data <- read.csv("pml-training.csv", header = TRUE)
assignment  <- read.csv('pml-testing.csv')
#store classe as factor
data$classe <- factor(data$classe)
data<-data[,-c(1,3,4,5,6,7)]
table(data$user_name)
table(assignment$user_name)
#split data in 60% training and 40% test
set.seed(9876)
trainIndex <- createDataPartition(y = data$classe, p=0.6,list=FALSE)
training <- data[trainIndex,]
testing<- data[-trainIndex,]
#Pre processing
NAColumn<- apply(training,2,function(x) {sum(is.na(x))})
preProcValues<-preProcess(training[,NAColumn>0], method="medianImpute")
trainProc <- predict(preProcValues, training)
testProc <- predict(preProcValues, testing)
nzv <- nearZeroVar(trainProc)
trainProc<- trainProc[, -nzv]
testProc<- testProc[, -nzv]
dummy<-dummyVars(~user_name, data=trainProc)
trainProc<-cbind(predict(dummy,trainProc),trainProc[-1])
testProc<-cbind(predict(dummy,testProc),testProc[-1])
minitrain<-trainTransformed[1:1000,]
minitrain<-trainProc[1:1000,]
registerDoParallel(makeCluster(detectCores()))
ctrl<- trainControl(method = "repeatedcv", number = 5, repeats = 5)
model<-train(classe ~ ., method = 'gbm', data =minitrain, trControl=ctrl)
registerDoParallel(makeCluster(detectCores()))
ctrl<- trainControl(method = "repeatedcv", number = 5, repeats = 5)
model<-train(classe ~ ., method = 'gbm', data =trainProc, trControl=ctr
l)
registerDoParallel(makeCluster(detectCores()))
ctrl<- trainControl(method = "repeatedcv", number = 100, repeats = 5)
model<-train(classe ~ ., method = 'gbm', data =trainProc, trControl=
ctrl)
registerDoParallel(makeCluster(detectCores()))
ctrl<- trainControl(method = "repeatedcv", number = 100, repeats = 5)
model<-train(classe ~ ., method = 'lda', data =trainProc, trControl=ctrl)
print(model)
set.seed(9876)
ctrl<- trainControl(method = "repeatedcv", number = 2, repeats = 5)
model2<-train(classe ~ ., method = 'rpart', data =trainProc, trControl=ctr
)
model<-train(classe ~ ., method = 'lda', data =trainProc, trControl=ctrl)
print(model)
model2<-train(classe ~ ., method = 'rpart', data =trainProc, trControl=ctrl)
print(model2)
model3<-train(classe ~ ., method = 'rf', data =trainProc, trControl=ctrl)
print(model3)
model1<-train(classe ~ ., method = 'lda', data =trainProc, trControl=ctrl)
accuracy<- predict(model3, trainProc)
confusionMatrix(accuracy, trainProc$classe))
confusionMatrix(accuracy, trainProc$classe)
predict(model3, trainProc)
accuracy<- predict(model3, testProc)
confusionMatrix(accuracy, test$classe))
accuracy<- predict(model3, testProc)
confusionMatrix(accuracy, test$classe)
accuracy<- predict(model3, testProc)
confusionMatrix(accuracy, testProc$classe)
accuracy<- predict(model1, testProc)
confusionMatrix(accuracy, testProc$classe)
confusionMatrix(accuracy, testProc$classe)$Accuracy
confusionMatrix(accuracy, testProc$classe)[2]
confusionMatrix(accuracy, testProc$classe)[3]
confusionMatrix(accuracy, testProc$classe)[3]$Accuracy
confusionMatrix(accuracy, testProc$classe)[3[1]]
confusionMatrix(accuracy, testProc$classe)$overall[1]
confusionMatrix(accuracy, testProc$classe)$overall$Accuracy
confusionMatrix(accuracy, testProc$classe)$overall[1]
accuracy<- predict(model2, testProc)
confusionMatrix(accuracy, testProc$classe)$overall[1]
print(model3)
model3$finalmodel
assignProc <- predict(preProcValues, assignment)
assignProc<- assignProc[, -nzv]
assignProc<-cbind(predict(dummy,assignProc),assignProc[-1])
pml_write_files = function(x){
n = length(x)
for(i in 1:n){
filename = paste0("problem_id_",i,".txt")
write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
}
}
prediction <- predict(model3, assignProc)
print(prediction)
answers <- as.vector(prediction)
pml_write_files(answers)
assignProc <- predict(preProcValues, assignment)
assignProc<-cbind(predict(dummy,assignProc),assignProc[-1])
prediction <- predict(model3, assignProc)
print(prediction)
answers <- as.vector(prediction)
pml_write_files(answers)
predict(model3, assignProc)[8]
predict(model2, assignProc)[8]
predict(model1, assignProc)[8
]
data <- read.csv("pml-training.csv", header = TRUE)
print(model1)
print(model1)[5]
print(model1)[4]
print(model1)[1]
print(1-model1[1])
as.numeric(model1[1])
model1[1]
model1[2]
model1[3]
model1[4]
model1%results[1]
model1$results[1]
model1$results[2]
paste(round(1-model1$results[2],2)*100,'%')
model2$result
max(model2$results[2],2)
max(model2$results[2])
model3$overall[2
]
model3$overall
model3$overal
model3
model3[4]
paste(round(1-max(model3$results[2]),2)*100,'%')
confusionMatrix(accuracy, testProc$classe)
prediction<- predict(model3, testProc)
confusionMatrix(prediction, testProc$classe)
confusionMatrix(prediction, testProc$classe)[1:2]
prediction<- predict(model3, testProc)
confusionMatrix(prediction, testProc$classe)[2:3]
prediction<- predict(model3, testProc)
confusionMatrix(prediction, testProc$classe)[2:3]
vi = varImp(model3$finalModel)
vi$var<-rownames(vi)
vi = as.data.frame(vi[with(vi, order(vi$Overall, decreasing=TRUE)), ])
rownames(vi) <- NULL
print(vi)
plot(vi)
plot(vi[1:5])
plot(vi[1:5])
vi[,1:5]
vi[,1:5]
vi
vi = varImp(model3$finalModel)
vi$var<-rownames(vi)
vi[,1:5]
vi
vi = vi[with(vi, order(vi$Overall, decreasing=TRUE)), ]
vi[1:5,]
plot(vi[1:5,1])
plot(vi, top = 5)
vi = varImp(model3$finalModel)
plot(vi, top = 5)
vi = varImp(model3)
plot(vi, top = 5)
vi = varImp(model3)
plot(vi, top = 10)
vi = varImp(model2)
plot(vi, top = 10)
testProc <-cbind(predict(dummy,testProc),testProc[-1])
library(caret)
library(parallel)
library(doParallel)
#load raw data and remove unecessary columns
data <- read.csv("pml-training.csv", header = TRUE)
data$classe <- factor(data$classe)
data<-data[,-c(1,3,4,5,6,7)]
#split data
set.seed(9876)
trainIndex <- createDataPartition(y = data$classe, p=0.6,list=FALSE)
training <- data[trainIndex,]
testing<- data[-trainIndex,]
#pre processing
NAColumn<- apply(training,2,function(x) {sum(is.na(x))})
preProcValues<-preProcess(training[,NAColumn>0], method="medianImpute")
trainProc1 <- predict(preProcValues, training)
nzv <- nearZeroVar(trainProc1)
trainProc2<- trainProc1[, -nzv]
dummy<-dummyVars(~user_name, data=trainProc)
trainProc<-cbind(predict(dummy,trainProc2),trainProc2[-1])
dummy<-dummyVars(~user_name, data=trainProc2)
trainProc<-cbind(predict(dummy,trainProc2),trainProc2[-1])
registerDoParallel(makeCluster(detectCores()))
set.seed(9876)
## 5 repeaed 2 fold CV
ctrl<- trainControl(method = "repeatedcv", number = 2, repeats = 5)
model1<-train(classe ~ ., method = 'lda', data =trainProc, trControl=ctrl)
model2<-train(classe ~ ., method = 'rpart', data =trainProc, trControl=ctrl)
model3<-train(classe ~ ., method = 'rf', data =trainProc, trControl=ctrl)
set.seed(7)
mns = NULL
for (i in 1 : 1000) mns = c(mns, mean(rexp(40))
)
library(ggplot2)
ggplot(dat, aes(x=mns)) + geom_density()
ggplot(aes(x=mns)) + geom_density()
ggplot(mns) + geom_density()
mns<-as.dataframe(mns)

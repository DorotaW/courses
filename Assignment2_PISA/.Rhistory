- "Data supports claim that if Kobe stops ball hogging the Lakers will win more"
- "Linear regression suggests that an increase of 1% in % of shots taken by Kobe results in a drop of 1.16 points (+/- 0.22)  in score differential."
- How was it done? Do you agree with the analysis?
---
## Questions for this class
* Consider trying to answer the following kinds of questions:
* To use the parents' heights to predict childrens' heights.
* To try to find a parsimonious, easily described mean
relationship between parent and children's heights.
* To investigate the variation in childrens' heights that appears
unrelated to parents' heights (residual variation).
* To quantify what impact genotype information has beyond parental height in explaining child height.
* To figure out how/whether and what assumptions are needed to
generalize findings beyond the data in question.
* Why do children of very tall parents tend to be
tall, but a little shorter than their parents and why children of very short parents tend to be short, but a little taller than their parents? (This is a famous question called 'Regression to the mean'.)
---
## Galton's Data
* Let's look at the data first, used by Francis Galton in 1885.
* Galton was a statistician who invented the term and concepts
of regression and correlation, founded the journal Biometrika,
and was the cousin of Charles Darwin.
* You may need to run `install.packages("UsingR")` if the `UsingR` library is not installed.
* Let's look at the marginal (parents disregarding children and children disregarding parents) distributions first.
* Parent distribution is all heterosexual couples.
* Correction for gender via multiplying female heights by 1.08.
* Overplotting is an issue from discretization.
---
```{r galton,fig.height=3.5,fig.width=8}
library(UsingR); data(galton); library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth=1)
g <- g + facet_grid(. ~ variable)
g
```
---
## Finding the middle via least squares
* Consider only the children's heights.
* How could one describe the "middle"?
* One definition, let $Y_i$ be the height of child $i$ for $i = 1, \ldots, n = 928$, then define the middle as the value of $\mu$
that minimizes $$\sum_{i=1}^n (Y_i - \mu)^2$$
* This is physical center of mass of the histrogram.
* You might have guessed that the answer $\mu = \bar Y$.
---
## Experiment
### Use R studio's manipulate to see what value of $\mu$ minimizes the sum of the squared deviations.
```
library(manipulate)
myHist <- function(mu){
mse <- mean((galton$child - mu)^2)
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mu, size = 3)
g <- g + ggtitle(paste("mu = ", mu, ", MSE = ", round(mse, 2), sep = ""))
g
}
manipulate(myHist(mu), mu = slider(62, 74, step = 0.5))
```
---
## The least squares est. is the empirical mean
```{r , fig.height=4, fig.width=4, fig.align='center'}
g <- ggplot(galton, aes(x = child)) + geom_histogram(fill = "salmon", colour = "black", binwidth=1)
g <- g + geom_vline(xintercept = mean(galton$child), size = 3)
g
```
---
### The math (not required for the class) follows as:
$$
\begin{align}
\sum_{i=1}^n \left(Y_i - \mu\right)^2 & = \
\sum_{i=1}^n \left(Y_i - \bar Y + \bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \sum_{i=1}^n \left(Y_i - \bar Y\right)  \left(\bar Y - \mu\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \left(\bar Y - \mu\right) \sum_{i=1}^n \left(Y_i - \bar Y\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
2 \left(\bar Y - \mu\right)  \left(\left(\sum_{i=1}^n Y_i\right) -\
n \bar Y\right) +\
\sum_{i=1}^n \left(\bar Y - \mu\right)^2 \\
& = \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 + \
\sum_{i=1}^n \left(\bar Y - \mu\right)^2\\
& \geq \sum_{i=1}^n \left(Y_i - \bar Y\right)^2 \
\end{align}
$$
---
## Comparing childrens' heights and their parents' heights
```{r, dependson="galton",fig.height=4,fig.width=4, fig.align='center'}
ggplot(galton, aes(x = parent, y = child)) + geom_point()
```
---
Size of point represents number of points at that (X, Y) combination (See the Rmd file for the code).
```{r freqGalton, dependson="galton",fig.height=6,fig.width=7,echo=FALSE}
library(dplyr)
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g
```
---
## Regression through the origin
* Suppose that $X_i$ are the parents' heights.
* Consider picking the slope $\beta$ that minimizes $$\sum_{i=1}^n (Y_i - X_i \beta)^2$$
* This is exactly using the origin as a pivot point picking the
line that minimizes the sum of the squared vertical distances
of the points to the line
* Use R studio's  manipulate function to experiment
* Subtract the means so that the origin is the mean of the parent
and children's heights
---
```{r, echo = TRUE, eval = FALSE}
y <- galton$child - mean(galton$child)
x <- galton$parent - mean(galton$parent)
freqData <- as.data.frame(table(x, y))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
myPlot <- function(beta){
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
g <- g + geom_abline(intercept = 0, slope = beta, size = 3)
mse <- mean( (y - beta * x) ^2 )
g <- g + ggtitle(paste("beta = ", beta, "mse = ", round(mse, 3)))
g
}
manipulate(myPlot(beta), beta = slider(0.6, 1.2, step = 0.02))
```
---
## The solution
### In the next few lectures we'll talk about why this is the solution
```{r}
lm(I(child - mean(child))~ I(parent - mean(parent)) - 1, data = galton)
```
---
```{r, fig.height=6,fig.width=7,echo=FALSE}
freqData <- as.data.frame(table(galton$child, galton$parent))
names(freqData) <- c("child", "parent", "freq")
freqData$child <- as.numeric(as.character(freqData$child))
freqData$parent <- as.numeric(as.character(freqData$parent))
g <- ggplot(filter(freqData, freq > 0), aes(x = parent, y = child))
g <- g  + scale_size(range = c(2, 20), guide = "none" )
g <- g + geom_point(colour="grey50", aes(size = freq+20, show_guide = FALSE))
g <- g + geom_point(aes(colour=freq, size = freq))
g <- g + scale_colour_gradient(low = "lightblue", high="white")
lm1 <- lm(galton$child ~ galton$parent)
g <- g + geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], size = 3, colour = grey(.5))
g
```
install.packages("UsingR")
library("UsingR", lib.loc="~/R/win-library/3.1")
library(UsingR); data(galton); library(reshape); long <- melt(galton)
g <- ggplot(long, aes(x = value, fill = variable))
g <- g + geom_histogram(colour = "black", binwidth=1)
g <- g + facet_grid(. ~ variable)
g
g <- ggplot(long, aes(x = value, fill = variable))
long <- melt(galton)
library(UsingR)
data(galton)
install.packages("reshape")
* Consider only the children's heights.
install.packages("dplyr")
library('knitr')
library('knitr')
install.packages("ggplot2")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text')
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
install.packages("shiny")
library(shiny)
shinyUI(pageWithSidebar(
headerPanel("Data science FTW!"),
sidebarPanel(
h2('Big text'),
h3('Sidebar')
),
mainPanel(
h3('Main Panel text')
)
))
x<- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
lsfit(x,w)
lsfit(x,wt=w)
x*w
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(I(x-mean(x))~I(y-mean(y)))
lm(y~x)
data(mtcarts)
data(mtcars)
view(mtcars)
d<-data(mtcars)
library("datasets", lib.loc="C:/Program Files/R/R-3.1.3/library")
d<-data(mtcars)
data(mtcars)
mtcars
lm(x~y)
lm(mpg~wt)
lm(mpg~wt,data=mtcars)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(stdev(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(std(a))
z<-c(8.58, 10.46, 9.01, 9.64, 8.86)
a<-z-mean(z)
a/(sd(a))
sum(x*w)/sum(w)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
mean(z)
mean(x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<lm(y~x)
f<-lm(y~x)
x <- c(0.61, 0.93, 0.83, 0.35, 0.54, 0.16, 0.91, 0.62, 0.62)
y <- c(0.67, 0.84, 0.6, 0.18, 0.85, 0.47, 1.1, 0.65, 0.36)
f<-lm(y~x)
summary(f)
mtcars
m<-lm(mpg~wt,data=mtcars)
summary(m)
-5.3445-0.55591*1.645
-5.3445-0.55591*1.645+37.2
(-5.3445-0.55591*1.645)*mean(mtcars.wt)+37.2
(-5.3445-0.55591*1.645)*mean(mtcars$wt)+37.2
(-5.3445+0.55591*1.645)*3000+37.2
(-5.3445+0.55591*1.645)*3+37.2
(-5.3445+0.55591*1.96)*3+37.2
(-5.3445)*3+37.2+1.645*3.046
(-5.3445)*mean(mtcars$wt)+37.2-1.645*3.046
predict(m, data.frame(mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=mean(mtcars$wt)), interval="confidence")
predict(m, data.frame(wt=3), interval="confidence")
summary(m)
(-5.3445)*2-2*1.645*0.5591
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(AppliedPredictiveModeling)
library(caret)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(training)
hist(training$SuperPlasticizer)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
mtcars<-mtcars
mtcars$cyl=factor(mtcars$cyl)
mtcars$vs=factor(mtcars$vs)
mtcars$gear=factor(mtcars$gear)
mtcars$carb=factor(mtcars$carb)
fit<-lm(mpg~cyl+wt)
fit<-lm(mpg~cyl+wt,data=mtcars)
summary(fit)
fit2<-lm(mpg~cyl,data=mtcars)
summary(fit2)
fit3<-lm(mpg~cyl+wt+cyk*wt,data=mtcars)
summary(fit3)
fit3<-lm(mpg~cyl+wt+cyl*wt,data=mtcars)
summary(fit3)
anova(fit, fit3)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
f<-lm(y~x)
hat(f)
hat(x)
f.x
sumary(f)
summary(f)
dfbetas(x)
dfbetas(f)
hat(cbind(y,x)
)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
data<-data(segmentationOriginal)
set.seed(125)
trainIndex <- createDataPartition(y = segmentationOriginal$Case, p=0.6,list=FALSE)
training <- segmentationOriginal[trainIndex,]
testing<- segmentationOriginal[-trainIndex,]
model2<-train(Case ~ ., method = 'rpart', data =training)
model2$finalmodel
model2
summary(model2)
model2$FinalModel
set.seed(125)
model2<-train(Case ~ ., method = 'rpart', data =training)
model2$finalModel
install.packages("pgmm")
library(pgmm)
data(olive)
olive = olive[,-1]
model<-train(Area ~ ., method = 'rpart', data =olive)
newdata = as.data.frame(t(colMeans(olive)))
predict(model,newdata)
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
predict(model,newdata)
model<-train(chd ~ age+ alcohol+obesity+tobacco+typea+ldl., method = 'glm', data =trainSA, family='binomial)
)
8
set.seed(13234)
model<-train(chd ~ age+ alcohol+obesity+tobacco+typea+ldl, method = 'glm', data =trainSA, family='binomial')
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA,model)
missClass(trainSA,predict(model,trainSA))
missClass(testSA$chd,predict(model,testSA))
missClass(trainSA$chd,predict(model,trainSA))
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y<-as.factor(vowel.test$y)
vowel.train$y<-as.factor(vowel.train$y)
set.seed(33833)
model3<-train(y ~ ., method = 'rf', data = vowel.train)
varImp(model3)
setwd("C:/Users/Dorota/Github/The_Analytics_Edge/Assignment2_Climate_Change")
data <- read.csv("climate_change.csv", header = TRUE)
training<-subset(data,year<=2006)
training<-subset(data,Year<=2006)
testing<-subset(data,Year>2006)
model<-lm(Temp ~ MEI + CO2 + CH4 + N2O + CFC.11 + CFC.12 + TSI + Aerosols, data=training)
summary(model)
cor(training)
corMatrix<-cor(training)
corMatrix[pval >0.7]
View(corMatrix)
which(corMatrix > 7, arr.ind = TRUE)
a<-which(corMatrix > 7, arr.ind = TRUE)
View(a)
which(corMatrix > 0.7, arr.ind = TRUE)
which(corMatrix > 0.7)
corMatrix[which(corMatrix > 0.7)]
corMatrix[which(corMatrix > 0.7),6]
corMatrix[which(corMatrix > 0.7)][6]
View(a)
which(corMatrix > 0.7, useName=TRUE)
which(corMatrix > 0.7, useNames=TRUE)
which(corMatrix > 0.7,  arr.ind=TRUE, useNames=TRUE)
which(corMatrix[,6] > 0.7,  arr.ind=TRUE, useNames=TRUE)
corMatrix[which(corMatrix[,6] > 0.7,  arr.ind=TRUE),6]
corMatrix[which(corMatrix[,8] > 0.7,  arr.ind=TRUE),8]
corMatrix[which(corMatrix[,7] > 0.7,  arr.ind=TRUE),7]
model2<-lm(Temp ~ MEI + N2O +  + TSI + Aerosols, data=training)
summary(model2)
model3<-step(model)
summary(model3)
predict(model3,testing)
prediction<-predict(model3,testing)
summary(prediction)
prediction
SS.total      <- sum((training$Temp - mean(training$Temp ))^2)
SS.regression <- sum((prediction - mean(training$Temp ))^2)
SS.regression/SS.total
prediction<-predict(model3,testing)
SS.total      <- sum((testing$Temp - mean(testing$Temp ))^2)
SS.regression <- sum((prediction - mean(testing$Temp ))^2)
SS.regression/SS.total
mean(testing$Temp )
prediction<-predict(model3,testing)
require(miscTools)
install.packages("miscTools")
require(miscTools)
r2 <- rSquared(testing, resid = tesing$Temp-prediction)
r2 <- rSquared(testing, resid = testing$Temp-prediction)
require(miscTools)
r2 <- rSquared(testing$Temp, resid = testing$Temp-prediction)
View(`r2`)
r2
cor(prediction, testing$Temp)^2
1−sum((testing$Temp−prediction)^2)/sum((testing$Temp−mean(testing$Temp))^2)
sum((testing$Temp−prediction)^2)/sum((testing$Temp−mean(testing$Temp))^2)
SSE <- sum((prediction - testing$Temp)^2)
SST <- sum( (mean(training$Temp) - testing$Temp)^2)
R2 <- 1 - SSE/SST
SSE <- sum((prediction - testing$Temp)^2)
SST <- sum( (mean(testing$Temp) - testing$Temp)^2)
R2 <- 1 - SSE/SST
setwd("C:/Users/Dorota/Github/The_Analytics_Edge/Assignment2_PISA")
pisaTrain<- read.csv("pisa2009train.csv", header = TRUE)
pisaTrain<- read.csv("pisa2009train.csv", header = TRUE)
pisaTest<- read.csv("pisa2009test.csv", header = TRUE)
tapply(pisaTrain,FUN=avg)
tapply(pisaTrain,FUN=mean)
tapply(pisaTrain,male,FUN=mean)
tapply(pisaTrain,pisaTrain$male,FUN=mean)
tapply(pisaTRain$readingScore,pisaTrain$male,FUN=mean)
tapply(pisaTrain$readingScore,pisaTrain$male,mean)
is.na(pisaTrain)
unlist(lapply(pisaTrain, function(x) any(is.na(x))))
colnames(pisaTRain)[unlist(lapply(pisaTrain, function(x) any(is.na(x))))]
colnames(pisaTrain)[unlist(lapply(pisaTrain, function(x) any(is.na(x))))]
colnames(pisaTrain)[-unlist(lapply(pisaTrain, function(x) any(is.na(x))))]
colSums(!is.na(pisaTrain) == 0)
pisaTrain = na.omit(pisaTrain)
pisaTest = na.omit(pisaTest)
pisaTrain$raceeth = relevel(pisaTrain$raceeth, "White")
pisaTest$raceeth = relevel(pisaTest$raceeth, "White")
lmScore<-lm(readingScore ~ ., data = pisaTrain)
summary(lmScore)
29.542707*2
rmse(lmScore)
RMSE=sqrt( mean( (lmScore$model-pisaTrain$readingScore)^2 , na.rm = TRUE ) )
RMSE
lmScore$model
head(lmScore$model)
RMSE=sqrt( mean( (lmScore$model[,1]-pisaTrain$readingScore)^2 , na.rm = TRUE ) )
RMSE
RMSE=sqrt( mean( (predict(lmScore,pisaTrain)-pisaTrain$readingScore)^2 , na.rm = TRUE ) )
RMSE
summary(lmScore)
RMSE=sqrt( mean( (lmScore$fitted.values-pisaTrain$readingScore)^2 , na.rm = TRUE ) )
RMSE
predTest<-predict(lmScore,pisaTest)
summary(predTest)
summary(predTest)[6]-summary(predTest)[1]
sqrt( mean( (predTest-pisaTest$readingScore)^2 , na.rm = TRUE )
)
sum((predTest- pisaTest$readingScore)^2)
sum( (mean(pisaTraining$readingScore) - pisaTest$readingScore)^2)
mean(pisaTrain$readingScore)
sum( (mean(pisaTrain$readingScore) - pisaTest$readingScore)^2)
